#!/bin/bash
set -euo pipefail
IFS=$'\n\t'

DAEDALUS_VERSION="0.3.2"


#======================#
# Handling dependancies
#======================#

DAEDALUS_ENV="daedalus"
ACMATCH_ENV="acmatch"

error() {
  echo "Daedalus error: $*" >&2
  exit 1
}

# ---- Conda bootstrap ----
if ! command -v conda >/dev/null 2>&1; then
  error "conda not found in PATH"
fi

# shellcheck disable=SC1091
source "$(conda info --base)/etc/profile.d/conda.sh" \
  || error "Failed to source conda.sh"

# ---- Check required environments ----
for ENV in "$DAEDALUS_ENV" "$ACMATCH_ENV"; do
  if ! conda env list | awk '{print $1}' | grep -qx "$ENV"; then
    error "Required conda environment '$ENV' does not exist"
  fi
done

# ---- Activate primary env ----
conda activate "$DAEDALUS_ENV" \
  || error "Failed to activate conda environment '$DAEDALUS_ENV'"

echo "Daedalus v$DAEDALUS_VERSION"
echo "Active env: $DAEDALUS_ENV ($(python --version))"


#=======================



# Default values
SRA_ID=""
NUM_CORES=1
MEM_GB=8
EPI_FASTA=""
READ1=""
READ2=""
FASTP_DIR="fastp_output"

# Help message
show_help() {
    echo "
Daedalus Pipeline

Usage:
  daedalus -e <epitope_fasta> -n <num_cores> -m <memory_gb> [--sra <SRA_ID>] [--read1 <read1.fastq.gz> --read2 <read2.fastq.gz>]

Flags:
  -e, --epitopes      Path to epitope FASTA file (required)
  -n, --num-cores     Number of cores (default: 1)
  -m, --memory        Memory in GB (default: 8)
  --sra <SRA_ID>      SRA accession ID (optional)
  --read1 <file>      Path to local R1 FASTQ file (required if no SRA)
  --read2 <file>      Path to local R2 FASTQ file (required if no SRA)
  --nohuman-db <path>   Path to nohuman database (or set $NOHUMAN_DB)
  -h, --help          Show this help message
  -V, --version       Print version and exit

Notes:
- If --sra is provided, local --read1 and --read2 are ignored.
- If no SRA is provided, both --read1 and --read2 must be specified.

Examples:
  daedalus --sra SRR123456 -e epitopes.fasta -n 32 -m 64
  daedalus --read1 sample_1.fastq.gz --read2 sample_2.fastq.gz -e epitopes.fasta -n 16 -m 32
"
}

# Parse flags
while [[ "$#" -gt 0 ]]; do
    case $1 in
        -n|--num-cores) NUM_CORES="$2"; shift ;;
        -m|--memory) MEM_GB="$2"; shift ;;
        -e|--epitopes) EPI_FASTA="$2"; shift ;;
        --sra) SRA_ID="$2"; shift ;;
        --read1) READ1="$2"; shift ;;
        --read2) READ2="$2"; shift ;;
        --nohuman-db) NOHUMAN_DB="$2"; shift ;;
        -h|--help) show_help; exit 0 ;;
        -V|--version) echo "daedalus ${DAEDALUS_VERSION}"; exit 0 ;;
        *) echo "Unknown parameter passed: $1"; show_help; exit 1 ;;
    esac
    shift
done




# Check required arguments
if [ -z "$EPI_FASTA" ]; then
    echo "ERROR: Epitope FASTA file (-e) is required."
    show_help
    exit 1
fi

if [ -n "$SRA_ID" ]; then
    echo "Using SRA accession: $SRA_ID"
elif [ -n "$READ1" ] && [ -n "$READ2" ]; then
    echo "Using local FASTQ files: $READ1, $READ2"
else
    echo "ERROR: Provide either --sra <SRA_ID> or both --read1 and --read2."
    show_help
    exit 1
fi


if [[ -z "${NOHUMAN_DB}" ]]; then
  echo "ERROR: nohuman DB not set. Use --nohuman-db /path/to/db or export NOHUMAN_DB=/path/to/db" >&2
  exit 1
fi
if [[ ! -e "${NOHUMAN_DB}" ]]; then
  echo "ERROR: nohuman DB not found at: ${NOHUMAN_DB}" >&2
  exit 1
fi


# ---------------------------
# Stage-aware resume / skip logic
# ---------------------------

mkdir -p "${FASTP_DIR}"

msg() { echo "[*] $*"; }
have() { [[ -s "$1" ]]; }   # file exists and non-empty

# ---------------------------
# Determine sample name early (for consistent output naming)
# ---------------------------
if [[ -n "$SRA_ID" ]]; then
  SAMPLE="$SRA_ID"
else
  # derive from READ1 basename: foo_1.fastq.gz -> foo
  BASE="$(basename "${READ1}")"
  SAMPLE="${BASE%%_*}"
fi

# ---------------------------
# Inputs: SRA download (skip if already present)
# ---------------------------
RAW_R1=""; RAW_R2=""; RAW_SE=""

if [[ -n "$SRA_ID" ]]; then
  mkdir -p sra_fastq

  # Accept either gz or plain fastq from prior runs
  SRA_R1_GZ="sra_fastq/${SRA_ID}_1.fastq.gz"
  SRA_R2_GZ="sra_fastq/${SRA_ID}_2.fastq.gz"
  SRA_SE_GZ="sra_fastq/${SRA_ID}.fastq.gz"

  SRA_R1="sra_fastq/${SRA_ID}_1.fastq"
  SRA_R2="sra_fastq/${SRA_ID}_2.fastq"
  SRA_SE="sra_fastq/${SRA_ID}.fastq"

  if have "$SRA_R1_GZ" && have "$SRA_R2_GZ"; then
    msg "SRA FASTQs already downloaded (PE); skipping fasterq-dump."
    RAW_R1="$SRA_R1_GZ"; RAW_R2="$SRA_R2_GZ"
  elif have "$SRA_SE_GZ"; then
    msg "SRA FASTQ already downloaded (SE); skipping fasterq-dump."
    RAW_SE="$SRA_SE_GZ"
  else
    msg "Downloading SRA via fasterq-dump..."
    fasterq-dump "$SRA_ID" --split-files -O sra_fastq

    # Compress if fasterq-dump produced plain .fastq
    if [[ -f "$SRA_R1" && -f "$SRA_R2" ]]; then
      msg "Compressing SRA FASTQs..."
      pigz -p "$NUM_CORES" -9 "$SRA_R1" "$SRA_R2"
      RAW_R1="$SRA_R1_GZ"; RAW_R2="$SRA_R2_GZ"
    elif [[ -f "$SRA_SE" ]]; then
      msg "Compressing SRA FASTQ..."
      pigz -p "$NUM_CORES" -9 "$SRA_SE"
      RAW_SE="$SRA_SE_GZ"
    else
      echo "ERROR: SRA download failed or produced no FASTQ files. Exiting."
      exit 1
    fi
  fi

else
  # Local fastqs
  if [[ ! -f "$READ1" || ! -f "$READ2" ]]; then
    echo "ERROR: One or both local FASTQ files not found. Exiting."
    exit 1
  fi
  RAW_R1="$READ1"; RAW_R2="$READ2"
fi

# ---------------------------
# Expected outputs for fastp + nohuman (we will reuse if nohuman outputs exist)
# ---------------------------
CLEAN_R1="${FASTP_DIR}/${SAMPLE}_R1.clean.fq.gz"
CLEAN_R2="${FASTP_DIR}/${SAMPLE}_R2.clean.fq.gz"
CLEAN_SE="${FASTP_DIR}/${SAMPLE}.clean.fq.gz"

NOHUMAN_R1="${FASTP_DIR}/${SAMPLE}_R1.nohuman.fq.gz"
NOHUMAN_R2="${FASTP_DIR}/${SAMPLE}_R2.nohuman.fq.gz"
NOHUMAN_SE="${FASTP_DIR}/${SAMPLE}.nohuman.fq.gz"
KRAKEN_REPORT="${FASTP_DIR}/${SAMPLE}.nohuman.kraken.report"

REPORT_HTML="${FASTP_DIR}/${SAMPLE}.fastp.html"
REPORT_JSON="${FASTP_DIR}/${SAMPLE}.fastp.json"

# ---------------------------
# nohuman step (skip if already done)
#   If nohuman outputs exist, we don't need to rerun fastp either.
# ---------------------------
if have "$NOHUMAN_R1" && have "$NOHUMAN_R2"; then
  msg "nohuman outputs found (PE); skipping fastp+nohuman."
elif have "$NOHUMAN_SE"; then
  msg "nohuman output found (SE); skipping fastp+nohuman."
else
  # Run fastp first (PE or SE)
  if [[ -n "${RAW_R1}" && -n "${RAW_R2}" ]]; then
    msg "Running fastp (PE)..."
    fastp -w "${NUM_CORES}" \
          -i "${RAW_R1}" -I "${RAW_R2}" \
          -o "${CLEAN_R1}" -O "${CLEAN_R2}" \
          --detect_adapter_for_pe \
          --qualified_quality_phred 15 \
          --length_required 50 \
          -h "${REPORT_HTML}" -j "${REPORT_JSON}"

    msg "Running nohuman (PE)..."
    nohuman \
      -t "${NUM_CORES}" \
      -D "${NOHUMAN_DB}" \
      -o "${NOHUMAN_R1}" \
      -O "${NOHUMAN_R2}" \
      -r "${KRAKEN_REPORT}" \
      "${CLEAN_R1}" "${CLEAN_R2}"

  elif [[ -n "${RAW_SE}" ]]; then
    msg "Running fastp (SE)..."
    fastp -w "${NUM_CORES}" \
          -i "${RAW_SE}" \
          -o "${CLEAN_SE}" \
          --qualified_quality_phred 15 --length_required 50 \
          -h "${REPORT_HTML}" -j "${REPORT_JSON}"

    msg "Running nohuman (SE)..."
    nohuman \
      -t "${NUM_CORES}" \
      -D "${NOHUMAN_DB}" \
      -o "${NOHUMAN_SE}" \
      -r "${KRAKEN_REPORT}" \
      "${CLEAN_SE}"
  else
    echo "ERROR: Input detection failed before fastp/nohuman."
    exit 1
  fi
fi

# ---------------------------
# MetaSPAdes (skip if scaffolds.fasta already present)
# ---------------------------
SPADES_DIR="spades_output"
SCAFFOLDS_FASTA="${SPADES_DIR}/scaffolds.fasta"

if have "$SCAFFOLDS_FASTA"; then
  msg "Found ${SCAFFOLDS_FASTA}; skipping MetaSPAdes."
else
  msg "Running MetaSPAdes..."
  mkdir -p "$SPADES_DIR"

  if have "$NOHUMAN_R1" && have "$NOHUMAN_R2"; then
    spades.py --meta \
      -1 "${NOHUMAN_R1}" \
      -2 "${NOHUMAN_R2}" \
      -o "$SPADES_DIR" \
      -t "${NUM_CORES}" \
      --memory "${MEM_GB}"
  elif have "$NOHUMAN_SE"; then
    spades.py --meta \
      -s "${NOHUMAN_SE}" \
      -o "$SPADES_DIR" \
      -t "${NUM_CORES}" \
      --memory "${MEM_GB}"
  else
    echo "ERROR: No nohuman reads available for SPAdes."
    exit 1
  fi

  # sanity: fail early if spades didn't produce scaffolds
  if [[ ! -s "$SCAFFOLDS_FASTA" ]]; then
    echo "ERROR: SPAdes did not produce scaffolds.fasta as expected."
    exit 1
  fi
fi

# ---------------------------
# Prodigal-GV (skip if proteins.faa already present)
# ---------------------------
PROTEINS_FAA="${SPADES_DIR}/proteins.faa"

if have "$PROTEINS_FAA"; then
  msg "Found ${PROTEINS_FAA}; skipping Prodigal-GV."
else
  msg "Running Prodigal-GV..."
  (
    cd "$SPADES_DIR"
    python3 /home/fryan/bin/parallel-prodigal-gv.py \
      -t "$NUM_CORES" -q \
      -i scaffolds.fasta \
      -a proteins.faa
  )

  if [[ ! -s "$PROTEINS_FAA" ]]; then
    echo "ERROR: Prodigal did not produce proteins.faa as expected."
    exit 1
  fi
fi

# ---------------------------
# Epitope search (Aho-Corasick)
# ---------------------------
msg "Aho-Corasick algorithm epitope search..."

EPITOPES="$EPI_FASTA"
PROTEINS="$PROTEINS_FAA"
OUT="ac_matches.tsv"

conda activate acmatch

JOBTAG="${SLURM_JOB_ID:-$$}"
WORKDIR="${MYSCRATCH:-$PWD}/acmatch_${JOBTAG}_${SAMPLE}"
mkdir -p "$WORKDIR"
cp "$PROTEINS" "$WORKDIR/proteins.faa"

ACMATCH_SCRIPT="$CONDA_PREFIX/share/daedalus/ac_match.py"
if [[ ! -f "$ACMATCH_SCRIPT" ]]; then
  echo "[ERROR] ac_match.py not found at $ACMATCH_SCRIPT"
  echo "Did you install Daedalus correctly?"
  exit 1
fi

python "$ACMATCH_SCRIPT" \
  --epitopes "$EPITOPES" \
  --proteins "$WORKDIR/proteins.faa" \
  --out "$OUT" \
  --min-len 8 \
  --max-len 30

rm -rf "$WORKDIR"

# ---------------------------
# Compress heavy outputs (safe with nullglob)
# ---------------------------
msg "Compressing outputs..."
conda activate daedalus

shopt -s nullglob
to_compress=( "${SPADES_DIR}"/*.fasta "${SPADES_DIR}"/*.faa "${SPADES_DIR}"/*.fastg "${SPADES_DIR}"/*.gfa )
if (( ${#to_compress[@]} > 0 )); then
  pigz -p "$NUM_CORES" -9 "${to_compress[@]}"
fi
shopt -u nullglob

msg "Daedalus pipeline completed successfully!"
